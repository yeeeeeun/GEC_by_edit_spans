{
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.14",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "sourceId": 10077893,
          "sourceType": "datasetVersion",
          "datasetId": 6212438
        },
        {
          "sourceId": 10078304,
          "sourceType": "datasetVersion",
          "datasetId": 6212762
        }
      ],
      "dockerImageVersionId": 30787,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yeeeeeun/GEC_by_edit_spans/blob/main/final_gec_propose.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from transformers import Trainer, TrainingArguments\n",
        "import json"
      ],
      "metadata": {
        "id": "Bn3nv9vmL0IC",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-02T14:15:43.366834Z",
          "iopub.execute_input": "2024-12-02T14:15:43.367516Z",
          "iopub.status.idle": "2024-12-02T14:15:43.371808Z",
          "shell.execute_reply.started": "2024-12-02T14:15:43.367482Z",
          "shell.execute_reply": "2024-12-02T14:15:43.370991Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GPU Î∂ÄÏ°±ÏúºÎ°ú Kaggle ÏÇ¨Ïö©\n"
      ],
      "metadata": {
        "id": "XxuRYmmhFknw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "FlokLWi0NVwP",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install accelerate>=0.20.1"
      ],
      "metadata": {
        "id": "A8hkGxrZUtH1",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# https://huggingface.co/j5ng/et5-base"
      ],
      "metadata": {
        "id": "gSe4XY0FQHT_"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, BartConfig,PreTrainedTokenizerFast\n",
        "import torch\n",
        "# Load the model and tokenizer\n",
        "model_name = 'gogamza/kobart-base-v2'\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "EeamwJAtO3Ek",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "referenced_widgets": [
            "8b6b60dd03ab4e35b887554e099eb35e",
            "65fce8841045477880cef7957445d4da",
            "db87391f1966410d81484d262d816b3e",
            "39cd5c0b57934df0b6a15c6c359df246",
            "24510edbca2345d29df5f4d233e593fb"
          ]
        },
        "outputId": "a1d1b6d7-5c16-43aa-f6cd-a5f6d9ea3588",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-02T14:15:13.040955Z",
          "iopub.execute_input": "2024-12-02T14:15:13.041270Z",
          "iopub.status.idle": "2024-12-02T14:15:19.780527Z",
          "shell.execute_reply.started": "2024-12-02T14:15:13.041242Z",
          "shell.execute_reply": "2024-12-02T14:15:19.779808Z"
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "config.json:   0%|          | 0.00/1.36k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8b6b60dd03ab4e35b887554e099eb35e"
            }
          },
          "metadata": {}
        },
        {
          "name": "stderr",
          "text": "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer.json:   0%|          | 0.00/682k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "65fce8841045477880cef7957445d4da"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "added_tokens.json:   0%|          | 0.00/4.00 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "db87391f1966410d81484d262d816b3e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "39cd5c0b57934df0b6a15c6c359df246"
            }
          },
          "metadata": {}
        },
        {
          "name": "stderr",
          "text": "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\nYou passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "model.safetensors:   0%|          | 0.00/495M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "24510edbca2345d29df5f4d233e593fb"
            }
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\")"
      ],
      "metadata": {
        "id": "hXi_ZRDzQJfw",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-02T14:15:19.781717Z",
          "iopub.execute_input": "2024-12-02T14:15:19.782107Z",
          "iopub.status.idle": "2024-12-02T14:15:19.786259Z",
          "shell.execute_reply.started": "2024-12-02T14:15:19.782061Z",
          "shell.execute_reply": "2024-12-02T14:15:19.785335Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "model = model.to(device)"
      ],
      "metadata": {
        "id": "FTAVA3m4U6KS",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-02T14:15:19.787381Z",
          "iopub.execute_input": "2024-12-02T14:15:19.787725Z",
          "iopub.status.idle": "2024-12-02T14:15:20.182380Z",
          "shell.execute_reply.started": "2024-12-02T14:15:19.787689Z",
          "shell.execute_reply": "2024-12-02T14:15:20.181568Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/kaggle/input/ffffffinal-dataset/propose_processed_data.json', \"r\", encoding=\"utf-8\") as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "df_sampled = df.sample(n=30000, random_state=42)\n",
        "\n",
        "x = df_sampled['input']\n",
        "y = df_sampled['output']\n",
        "\n",
        "train_df, test_df = train_test_split(df_sampled, train_size=20000, test_size=10000, random_state=42)\n",
        "train_df, val_df = train_test_split(train_df, test_size=0.2, random_state=42)\n",
        "\n",
        "x_train = train_df['input']\n",
        "x_valid = val_df['input']\n",
        "y_train = train_df['output']\n",
        "y_valid = val_df['output']\n",
        "\n",
        "print(\"Train Îç∞Ïù¥ÌÑ∞ ÌÅ¨Í∏∞:\", train_df.shape[0])\n",
        "print(\"Valid Îç∞Ïù¥ÌÑ∞ ÌÅ¨Í∏∞:\", val_df.shape[0])\n",
        "print(\"Test Îç∞Ïù¥ÌÑ∞ ÌÅ¨Í∏∞:\", test_df.shape[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hm775KnlQfGM",
        "outputId": "58694614-5f48-4e6e-87d3-23797f7e3ff7",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-02T16:35:16.784195Z",
          "iopub.execute_input": "2024-12-02T16:35:16.784562Z",
          "iopub.status.idle": "2024-12-02T16:35:17.380885Z",
          "shell.execute_reply.started": "2024-12-02T16:35:16.784532Z",
          "shell.execute_reply": "2024-12-02T16:35:17.379976Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Train Îç∞Ïù¥ÌÑ∞ ÌÅ¨Í∏∞: 16000\nValid Îç∞Ïù¥ÌÑ∞ ÌÅ¨Í∏∞: 4000\nTest Îç∞Ïù¥ÌÑ∞ ÌÅ¨Í∏∞: 10000\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "KreNvbiPVOFv",
        "outputId": "8efb297e-b140-44a7-a90e-d2e27b3e56b9",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-02T14:22:59.893499Z",
          "iopub.execute_input": "2024-12-02T14:22:59.893845Z",
          "iopub.status.idle": "2024-12-02T14:22:59.900671Z",
          "shell.execute_reply.started": "2024-12-02T14:22:59.893808Z",
          "shell.execute_reply": "2024-12-02T14:22:59.899795Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "                        input                                     output\n118839    Íº≠ ÏïÑÏπ®Ïóê Î∞• Î®πÍ≥† ÏïåÎ∞îÍ∞ÄÎ¶¨~!!!  (13, 13, ' '), (14, 15, ''), (17, 19, '')\n79677   ÏïÑÎ™¨Îìú ÌéòÌéòÎäî Ïù¥ ÏÇºÏùºÏóê Ï£ºÎùºÍ≥† ÌïòÎìúÎùº  (9, 10, ''), (19, 20, 'Îçî'), (21, 21, '.')\n215391                Îì±Îì± ÎßéÏù¥Ï£ºÎäî                                (5, 5, ' ')\n78682                 ÏïºÏ±ÑÏ£Ω...?                                 (3, 6, '')\n176584                     ÎÑ§ÎÑ§                                (2, 2, '.')\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "max_length = df_sampled.iloc[:, 0].str.len().max()\n",
        "\n",
        "\n",
        "longest_sentence = df_sampled.loc[df_sampled.iloc[:, 0].str.len() == max_length, df_sampled.columns[0]].values[0]\n",
        "\n",
        "print(\"Í∞ÄÏû• Í∏¥ Î¨∏Ïû•Ïùò Í∏ÄÏûêÏàò:\", max_length)\n",
        "print(\"Í∞ÄÏû• Í∏¥ Î¨∏Ïû•ÏùÑ Ï∂úÎ†•Ìï®:\", longest_sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c50eSX6TVSx3",
        "outputId": "4418b05a-b67a-416e-a20a-19fae3903a0b",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-02T14:23:00.806194Z",
          "iopub.execute_input": "2024-12-02T14:23:00.806514Z",
          "iopub.status.idle": "2024-12-02T14:23:00.838786Z",
          "shell.execute_reply.started": "2024-12-02T14:23:00.806489Z",
          "shell.execute_reply": "2024-12-02T14:23:00.837952Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Í∞ÄÏû• Í∏¥ Î¨∏Ïû•Ïùò Í∏ÄÏûêÏàò: 477\nÍ∞ÄÏû• Í∏¥ Î¨∏Ïû•ÏùÑ Ï∂úÎ†•Ìï®: Í∑∏Îü¨Î©¥ÏÑúÎèÑ ÏûêÍ∏∞Í∞Ä Îî∏ÏóêÍ≤å Îã§Ïãú ÏûòÏù¥ÏïºÍ∏∞ Ìï¥Î≥¥Ïã†Îã§Î©∞ Ïó¨Ïö¥ÏùÑ ÎÇ®Í∏∞ÏãúÍ∏∏Îûò ÏßëÏò§Î©¥ÏÑú ÎèÑÏôÄÎã¨ÎùºÍ≥† ÎßêÏîÄÏùÑ ÎìúÎ†∏Í≥†, Îã§ÏùåÎÇ†Ïóî Ï†ÄÎ†áÍ≤å Ïπ¥ÌÜ°ÏùÑ Î≥¥ÎÇ¥ÏãúÍ∏∏Îûò Ï†úÍ∞Ä Ïù¥Îü∞ Î¨∏ÏûêÎ•º Î≥¥ÎÉàÍ±∞Îì†\nÎÑ§. Ïñ¥Î®∏Îãà ÎßêÏîÄ ÏûòÏù¥Ìï¥ÌñàÏäµÎãàÎã§. Ïñ¥Ï†ú Ï†úÍ≤å ÏùºÎßêÏùò Ïó¨Ïö¥ÏùÑ ÎÇ®Í∏∞ÏßÄ ÏïäÏúºÏÖ®Îã§Î©¥ ÌïòÎ£®ÎçîÎπ®Î¶¨ ÎßàÏùåÏ†ïÎ¶¨ Ìï† Ïàò ÏûàÏóàÏùÑÍªçÎãàÎã§.  Ïñ¥Î®∏ÎãàÍªòÏÑú Ï¢ãÏùÄÎßàÏùå Í∞ñÍ≥† ÎèÑÏôÄÏ£ºÏãúÍ≤†Îã§Í≥† ÌïòÏãúÏñ¥ ÎìúÎ¶∞ÎßêÏîÄÏù¥ÏßÄ ÏßëÏ∞©ÏùÄ ÏïÑÎãàÏóàÏäµÎãàÎã§. Îß§Ï£º Îî∞ÎãòÍ≥º Ï†ÄÌù¨ ÏßëÏóêÏÑú Ìï®ÍªòÏßÄÎÇ¥Í≥†, Îòê Îß§Ïùº Î∂ôÏñ¥ ÏûàÎã§Ïã∂Ïù¥ ÌñàÏäµÎãàÎã§. ÎïåÎ°† Ïñ¥Î®∏ÎãàÍªò Í±∞ÏßìÎßêÏùÑÌïòÍ≥† Ïó¨ÌñâÏùÑ Í∞ÄÍ∏∞ÎèÑ ÌñàÏäµÎãàÎã§. Ïù¥Ï†† ÏÑúÎ°ú ÎßàÏùåÏù¥ ÍπäÏñ¥Ï°åÍ≥† Í∑∏ ÍπäÏñ¥ÏßÑ ÎßàÏùå Î≥¥Ïó¨ÎìúÎ¶¨Î†§ÌñàÏùÑÎøê ÌòÑÏã§Ï†ÅÏù∏Í≤ÉÏùÑ ÎßêÏîÄÎìúÎ¶¨Î†§ÌïúÍ±¥ ÏïÑÎãôÎãàÎã§. ÎèàÏù¥ Ï§ëÏöîÌïúÍ±∞ ÏïåÍ≥† ÏïàÏ†ïÏ†ÅÏù∏ ÏÉùÌôúÏù¥ Ï§ëÏöîÌïúÍ±¥ ÏïïÎãàÎã§. Ï†úÍ∞Ä Î∂ÄÎ™®ÎãòÍªò ÎèÑÏõÄÏùÑ Î∞õÏïòÏúºÎ©¥ Î∞õÏïòÏßÄ ÎìúÎ¶¨Îäî ÌòïÌé∏ÏïÑÎãôÎãàÎã§. Î∂ÄÎ™®ÎãòÏùÄ Ïñ∏Î°†Ïù∏ Ï∂úÏã†Ïù¥ÏãúÍ≥† Í≥µÎ¨¥ÏõêÏÉùÌôúÎèÑ Ïò§ÎûòÌïòÏã† Î∂ÑÎì§ ÏûÖÎãàÎã§. ÌòÑÏã§Î≥¥Îã§ Í∞êÏ†ïÏù¥ Í∏∞Î∞òÎêú ÏÇ¨ÎûëÏùÑ ÌïòÍ≥†Ïã∂ÏóàÏùÑ ÎøêÏù¥ÏóêÏöî. ÏïÑÏâΩÏßÄÎßå Ï†ÄÎäî Ïù¥Îßå ÌïòÎ†µÎãàÎã§. ÏûòÏßÄÎÇ¥ÏÑ∏Ïöî.\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "class GECDatasets(Dataset):\n",
        "    def __init__(self, data, tokenizer, max_length=128):\n",
        "        self.data = data\n",
        "        self.input = [example['input'] for example in data]\n",
        "        self.output = [example['output'] for example in data]  # ÏàòÏ†ï Ï†ïÎ≥¥\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        input_text = self.input[idx]\n",
        "        edits = self.output[idx]\n",
        "\n",
        "\n",
        "        encoding = self.tokenizer(input_text, truncation=True, padding='max_length', max_length=self.max_length, return_tensors='pt')\n",
        "        input_ids = encoding['input_ids'].squeeze(0)\n",
        "        attention_mask = encoding['attention_mask'].squeeze(0)\n",
        "\n",
        "        output_encoding = self.tokenizer(edits, truncation=True, padding='max_length', max_length=self.max_length, return_tensors='pt')\n",
        "        labels = output_encoding['input_ids'].squeeze(0)\n",
        "\n",
        "        sample = {\n",
        "            'input_ids': input_ids,\n",
        "            'attention_mask': attention_mask,\n",
        "            'labels': labels\n",
        "        }\n",
        "\n",
        "        return sample\n"
      ],
      "metadata": {
        "id": "HKfiydB0Y4rs",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-02T14:23:02.548968Z",
          "iopub.execute_input": "2024-12-02T14:23:02.549585Z",
          "iopub.status.idle": "2024-12-02T14:23:02.556822Z",
          "shell.execute_reply.started": "2024-12-02T14:23:02.549554Z",
          "shell.execute_reply": "2024-12-02T14:23:02.555896Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = GECDatasets(data=df_sampled.to_dict(orient='records'), tokenizer=tokenizer)"
      ],
      "metadata": {
        "id": "haNSOJBeY6Ax",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-02T14:23:04.298424Z",
          "iopub.execute_input": "2024-12-02T14:23:04.299236Z",
          "iopub.status.idle": "2024-12-02T14:23:04.390214Z",
          "shell.execute_reply.started": "2024-12-02T14:23:04.299203Z",
          "shell.execute_reply": "2024-12-02T14:23:04.389273Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0XMZs5DVg-T0",
        "outputId": "b16617d0-173d-4ef5-909e-bc8397b04463",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-02T14:23:05.626384Z",
          "iopub.execute_input": "2024-12-02T14:23:05.626756Z",
          "iopub.status.idle": "2024-12-02T14:23:05.635222Z",
          "shell.execute_reply.started": "2024-12-02T14:23:05.626727Z",
          "shell.execute_reply": "2024-12-02T14:23:05.634331Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "{'input_ids': tensor([14427,  9866, 20141, 12258, 14304,   214,     3,     3,     3,     3,\n            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n            3,     3,     3,     3,     3,     3,     3,     3]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0]), 'labels': tensor([14338,   255,   243, 14260,   243, 14063,   245, 14063,   240,     3,\n            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n            3,     3,     3,     3,     3,     3,     3,     3])}\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(dataset))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0W33OuKJnJZR",
        "outputId": "2275e6c8-7097-46c0-d2f6-307afdbc5211",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-02T14:23:14.411698Z",
          "iopub.execute_input": "2024-12-02T14:23:14.412064Z",
          "iopub.status.idle": "2024-12-02T14:23:14.416698Z",
          "shell.execute_reply.started": "2024-12-02T14:23:14.412033Z",
          "shell.execute_reply": "2024-12-02T14:23:14.415897Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "30000\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = []\n",
        "for input_text, edits in zip(x_train.values, y_train.values):\n",
        "    train_data.append({\n",
        "        'input': input_text,  # ÏûòÎ™ªÎêú Î¨∏Ïû•\n",
        "        'output': edits        # ÏàòÏ†ï Ï†ïÎ≥¥\n",
        "    })\n",
        "\n",
        "valid_data = []\n",
        "for input_text, edits in zip(x_valid.values, y_valid.values):\n",
        "    valid_data.append({\n",
        "        'input': input_text,  # ÏûòÎ™ªÎêú Î¨∏Ïû•\n",
        "        'output': edits        # ÏàòÏ†ï Ï†ïÎ≥¥\n",
        "    })\n",
        "\n",
        "train_dataset = GECDatasets(train_data, tokenizer)\n",
        "valid_dataset = GECDatasets(valid_data, tokenizer)"
      ],
      "metadata": {
        "id": "9B3N5-tHjQ6t",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-02T14:23:22.792001Z",
          "iopub.execute_input": "2024-12-02T14:23:22.792837Z",
          "iopub.status.idle": "2024-12-02T14:23:22.814897Z",
          "shell.execute_reply.started": "2024-12-02T14:23:22.792804Z",
          "shell.execute_reply": "2024-12-02T14:23:22.814102Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import EarlyStoppingCallback\n",
        "\n",
        "# EarlyStoppingCallback ÏÑ§Ï†ï\n",
        "early_stopping = EarlyStoppingCallback(\n",
        "    early_stopping_patience=2,\n",
        "    early_stopping_threshold=0.0,\n",
        ")\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=5e-5,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    num_train_epochs=10,\n",
        "    weight_decay=0.01,\n",
        "    save_total_limit=1,\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=50,\n",
        "    report_to=\"none\",\n",
        "    load_best_model_at_end=True,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=valid_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    callbacks=[early_stopping],\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 167
        },
        "id": "ELftuDVpjKG7",
        "outputId": "f21381ef-e9ef-4b45-bc24-cb077b51003a",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-02T14:24:08.706371Z",
          "iopub.execute_input": "2024-12-02T14:24:08.706742Z",
          "iopub.status.idle": "2024-12-02T15:00:38.338623Z",
          "shell.execute_reply.started": "2024-12-02T14:24:08.706709Z",
          "shell.execute_reply": "2024-12-02T15:00:38.337633Z"
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n  warnings.warn(\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='12000' max='20000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [12000/20000 36:27 < 24:18, 5.48 it/s, Epoch 6/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.078800</td>\n      <td>0.076171</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.067400</td>\n      <td>0.065905</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.053900</td>\n      <td>0.060678</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.043300</td>\n      <td>0.058898</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.032700</td>\n      <td>0.060018</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>0.026800</td>\n      <td>0.065547</td>\n    </tr>\n  </tbody>\n</table><p>"
          },
          "metadata": {}
        },
        {
          "name": "stderr",
          "text": "/opt/conda/lib/python3.10/site-packages/transformers/modeling_utils.py:2618: UserWarning: Moving the following attributes in the config to the generation config: {'forced_eos_token_id': 1}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n  warnings.warn(\nThere were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.decoder.embed_tokens.weight', 'lm_head.weight'].\n",
          "output_type": "stream"
        },
        {
          "execution_count": 27,
          "output_type": "execute_result",
          "data": {
            "text/plain": "TrainOutput(global_step=12000, training_loss=0.06755638842781385, metrics={'train_runtime': 2188.2691, 'train_samples_per_second': 73.117, 'train_steps_per_second': 9.14, 'total_flos': 7316837498880000.0, 'train_loss': 0.06755638842781385, 'epoch': 6.0})"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "wrong_input = 'Í∑∏Îäî ÏÇ¨Í≥ºÏùÑ Î®πÍ≥†ÏûàÎã§.'\n",
        "\n",
        "input_ids = tokenizer(wrong_input, return_tensors='pt').input_ids\n",
        "edit_out = model.generate(input_ids, max_length=128, num_beams=4, length_penalty=2.0, early_stopping=True)\n",
        "corrected_output = tokenizer.decode(edit_out[0], skip_special_tokens=True)\n",
        "\n",
        "edits = get_edit_operations(wrong_input, corrected_output)\n",
        "print(edits)"
      ],
      "metadata": {
        "id": "_qwsI7-YjpQq"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), './final_gec_model')"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-02T15:02:14.875162Z",
          "iopub.execute_input": "2024-12-02T15:02:14.875763Z",
          "iopub.status.idle": "2024-12-02T15:02:15.585449Z",
          "shell.execute_reply.started": "2024-12-02T15:02:14.875729Z",
          "shell.execute_reply": "2024-12-02T15:02:15.584457Z"
        },
        "id": "iyR3L39lFimj"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from IPython.display import FileLink\n",
        "import os\n",
        "\n",
        "result = np.array([0.0])\n",
        "np.save(\"/kaggle/working/embedding\",result)\n",
        "os.chdir(r'/kaggle/working')\n",
        "FileLink(r'./final_gec_model')"
      ],
      "metadata": {
        "trusted": true,
        "id": "hcKnGSa9Fimj"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import BartForConditionalGeneration, BartTokenizer\n",
        "\n",
        "model_name = 'gogamza/kobart-base-v2'\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "\n",
        "model.load_state_dict(torch.load('/kaggle/working/final_gec_model', map_location=torch.device('cpu')))\n",
        "model.eval()\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "\n",
        "print(\"Î¨∏Ïû•ÏùÑ ÏûÖÎ†•ÌïòÏÑ∏Ïöî:\")\n",
        "input_text = input()\n",
        "\n",
        "input_encoding = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
        "\n",
        "output_encoding = model.generate(\n",
        "    input_ids=input_encoding.input_ids,\n",
        "    attention_mask=input_encoding.attention_mask,\n",
        "    max_length=128,\n",
        "    num_beams=5,\n",
        ")\n",
        "\n",
        "output_text = tokenizer.decode(output_encoding[0], skip_special_tokens=True)\n",
        "\n",
        "print(\"edit spans: \", output_text)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-02T16:49:36.625027Z",
          "iopub.execute_input": "2024-12-02T16:49:36.625724Z",
          "iopub.status.idle": "2024-12-02T16:49:46.054630Z",
          "shell.execute_reply.started": "2024-12-02T16:49:36.625692Z",
          "shell.execute_reply": "2024-12-02T16:49:46.053699Z"
        },
        "id": "zwSD9r8hFimj",
        "outputId": "9de658e3-a31d-415f-f80a-d41d5df4b726"
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\nYou passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\nYou passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n/tmp/ipykernel_30/3546478252.py:10: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load('/kaggle/working/final_gec_model', map_location=torch.device('cpu')))\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Î¨∏Ïû•ÏùÑ ÏûÖÎ†•ÌïòÏÑ∏Ïöî:\n",
          "output_type": "stream"
        },
        {
          "output_type": "stream",
          "name": "stdin",
          "text": " ÏôîÏñ¥Ïö©?\n"
        },
        {
          "name": "stdout",
          "text": "edit spans:  (2, 3, 'Ïöî')\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install m2-score\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-02T16:27:01.610485Z",
          "iopub.execute_input": "2024-12-02T16:27:01.611308Z",
          "iopub.status.idle": "2024-12-02T16:27:03.418082Z",
          "shell.execute_reply.started": "2024-12-02T16:27:01.611275Z",
          "shell.execute_reply": "2024-12-02T16:27:03.417082Z"
        },
        "id": "SsfIzsz1Fimj",
        "outputId": "7ee60ff0-d043-4f34-e2f9-8894d9509550"
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\u001b[31mERROR: Could not find a version that satisfies the requirement m2-score (from versions: none)\u001b[0m\u001b[31m\n\u001b[0m\u001b[31mERROR: No matching distribution found for m2-score\u001b[0m\u001b[31m\n\u001b[0m",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from tqdm import tqdm\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "import pandas as pd\n",
        "import time\n",
        "import ast\n",
        "\n",
        "model_name = 'gogamza/kobart-base-v2'\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "\n",
        "model.load_state_dict(torch.load('/kaggle/working/final_gec_model', map_location=torch.device('cpu')))\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "test_df = test_df.sample(n=1000, random_state=42)\n",
        "\n",
        "def apply_edit_spans(input_sentence, edit_spans):\n",
        "\n",
        "    input_sentence = list(input_sentence)\n",
        "    try:\n",
        "        for span in sorted(edit_spans, key=lambda x: x[0], reverse=True):\n",
        "            start, end, replacement = span\n",
        "            input_sentence[start:end] = list(replacement)\n",
        "    except Exception as e:\n",
        "        print(f\"Error in apply_edit_spans: {e}\")\n",
        "        print(f\"edit_spans: {edit_spans}\")\n",
        "        raise e\n",
        "    return ''.join(input_sentence)\n",
        "\n",
        "def calculate_bleu(model, tokenizer, dataset, max_length=128):\n",
        "    references = []\n",
        "    hypotheses = []\n",
        "\n",
        "    start_time = time.time()\n",
        "    with torch.no_grad():\n",
        "        for _, row in tqdm(dataset.iterrows(), desc=\"Calculating BLEU\", total=len(dataset)):\n",
        "            source_sentence = row['input']\n",
        "            target_sentence = row['output']\n",
        "\n",
        "            inputs = tokenizer(source_sentence, return_tensors=\"pt\", padding=True, truncation=True, max_length=max_length).to(device)\n",
        "\n",
        "            output = model.generate(input_ids=inputs['input_ids'], attention_mask=inputs['attention_mask'], max_length=max_length)\n",
        "            raw_edit_spans = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "            try:\n",
        "                edit_spans = ast.literal_eval(f\"[{raw_edit_spans}]\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error parsing edit spans: {e}\")\n",
        "                print(f\"raw_edit_spans: {raw_edit_spans}\")\n",
        "                continue\n",
        "\n",
        "            corrected_sentence = apply_edit_spans(source_sentence, edit_spans)\n",
        "\n",
        "            references.append([target_sentence.split()])\n",
        "            hypotheses.append(corrected_sentence.split())\n",
        "\n",
        "    prediction_time = time.time() - start_time\n",
        "    bleu_score = corpus_bleu(references, hypotheses)\n",
        "    return bleu_score, prediction_time\n",
        "\n",
        "bleu_score, prediction_time = calculate_bleu(model, tokenizer, test_df)\n",
        "print(f\"BLEU Score: {bleu_score}\")\n",
        "print(f\"Time: {prediction_time}\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-02T16:35:20.988698Z",
          "iopub.execute_input": "2024-12-02T16:35:20.989079Z",
          "iopub.status.idle": "2024-12-02T16:46:21.285474Z",
          "shell.execute_reply.started": "2024-12-02T16:35:20.989048Z",
          "shell.execute_reply": "2024-12-02T16:46:21.284583Z"
        },
        "id": "1qjnEe2dFimk",
        "outputId": "5f59a8df-16c3-4ab9-b4cd-a0c9419732ba"
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\nYou passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\nYou passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n/tmp/ipykernel_30/2550538412.py:15: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load('/kaggle/working/final_gec_model', map_location=torch.device('cpu')))\nCalculating BLEU:   4%|‚ñç         | 38/1000 [00:25<10:31,  1.52it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Error parsing edit spans: invalid syntax (<unknown>, line 1)\nraw_edit_spans: (3,, ' '), (15, 15, ' '), (17, 17, ' '), (22, 22, '. ')\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Calculating BLEU:   8%|‚ñä         | 76/1000 [00:50<10:11,  1.51it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Error parsing edit spans: invalid syntax (<unknown>, line 1)\nraw_edit_spans: (3,, ' '), (4, 4, ' '), (6, 6, ' '), (11, 11, ' '), (13, 13, ' '), (16, 16, ' '), (17, 17, '.')\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Calculating BLEU:  13%|‚ñà‚ñé        | 126/1000 [01:22<09:39,  1.51it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Error parsing edit spans: invalid syntax (<unknown>, line 1)\nraw_edit_spans: (3,, ' '), (5, 6, ''), (7, 7, '.')\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Calculating BLEU:  30%|‚ñà‚ñà‚ñâ       | 299/1000 [03:16<07:41,  1.52it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Error parsing edit spans: closing parenthesis ']' does not match opening parenthesis '(' (<unknown>, line 1)\nraw_edit_spans: (3, 4, 'ÏÉÄ'), (6, 7, ' Ïûà'), (8, 8, '.'), (11, 11, ' '), (13, 13, ' '), (15, 15, ' '), (17, 17, ' '), (22, 22, ' '), (24, 24, ' '), (26, 26, ' '), (27, 27, ' '), (31, 31, ' '), (32, 32, ' '), (34, 34, ' '), (36, 36, ' '), (36, 36\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Calculating BLEU:  30%|‚ñà‚ñà‚ñà       | 300/1000 [03:17<07:39,  1.52it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Error parsing edit spans: unterminated string literal (detected at line 1) (<unknown>, line 1)\nraw_edit_spans: (3, 4, '3, 4, 'Îäî'), (5, 5, ' '), (7, 7, '.')\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Calculating BLEU:  31%|‚ñà‚ñà‚ñà‚ñè      | 313/1000 [03:26<07:29,  1.53it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Error parsing edit spans: invalid syntax (<unknown>, line 1)\nraw_edit_spans: (3,, ' '), (6, 6, '.')\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Calculating BLEU:  33%|‚ñà‚ñà‚ñà‚ñé      | 334/1000 [03:40<07:18,  1.52it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Error parsing edit spans: invalid syntax (<unknown>, line 1)\nraw_edit_spans: (1,, ' Í±∞'), (3, 4, ' Í±∞'), (5, 5, '.')\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Calculating BLEU:  34%|‚ñà‚ñà‚ñà‚ñç      | 344/1000 [03:46<07:04,  1.55it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Error parsing edit spans: invalid syntax (<unknown>, line 1)\nraw_edit_spans: (3,, ' ')\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Calculating BLEU:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 477/1000 [05:13<05:42,  1.53it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Error parsing edit spans: invalid syntax (<unknown>, line 1)\nraw_edit_spans: (3,, '.')\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Calculating BLEU:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 512/1000 [05:37<05:19,  1.53it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Error parsing edit spans: invalid syntax. Perhaps you forgot a comma? (<unknown>, line 1)\nraw_edit_spans: (5 2, 'Ï∞Æ'), (6, 6, ',')\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Calculating BLEU:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 537/1000 [05:53<05:01,  1.53it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Error parsing edit spans: invalid syntax. Perhaps you forgot a comma? (<unknown>, line 1)\nraw_edit_spans: (4 3, ','), (7, 7, ' ')\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Calculating BLEU:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 541/1000 [05:56<05:00,  1.53it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Error parsing edit spans: invalid syntax. Perhaps you forgot a comma? (<unknown>, line 1)\nraw_edit_spans: (7 2, 'Î∞ú'), (7, 7, ' '), (9, 9, '.')\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Calculating BLEU:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 557/1000 [06:06<04:49,  1.53it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Error parsing edit spans: invalid syntax (<unknown>, line 1)\nraw_edit_spans: (1,, 'ÏõÉ'), (14, 14, ' '), (17, 18, '')\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Calculating BLEU:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 599/1000 [06:34<04:20,  1.54it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Error parsing edit spans: invalid syntax. Perhaps you forgot a comma? (<unknown>, line 1)\nraw_edit_spans: (4 4, '')\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Calculating BLEU:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 604/1000 [06:37<04:21,  1.51it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Error parsing edit spans: invalid syntax (<unknown>, line 1)\nraw_edit_spans: (3,, ' '), (4, 4, ' '), (22, 22, '.')\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Calculating BLEU:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 631/1000 [06:55<04:00,  1.53it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Error parsing edit spans: invalid syntax (<unknown>, line 1)\nraw_edit_spans: (1,, ''), (22, 22, ' ')\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Calculating BLEU:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 667/1000 [07:19<03:35,  1.55it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Error parsing edit spans: invalid syntax (<unknown>, line 1)\nraw_edit_spans: (2,, 'Îü¨'), (27, 27, '.')\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Calculating BLEU:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 680/1000 [07:27<03:27,  1.54it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Error parsing edit spans: invalid syntax (<unknown>, line 1)\nraw_edit_spans: (2,, '')\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Calculating BLEU:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 719/1000 [07:53<03:02,  1.54it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Error parsing edit spans: invalid syntax (<unknown>, line 1)\nraw_edit_spans: (3,, '.')\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Calculating BLEU:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 754/1000 [08:16<02:42,  1.52it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Error parsing edit spans: invalid syntax (<unknown>, line 1)\nraw_edit_spans: (2,, ''), (22, 22, ' ')\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Calculating BLEU:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 802/1000 [08:47<02:10,  1.52it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Error parsing edit spans: invalid syntax (<unknown>, line 1)\nraw_edit_spans: (1,, 'Î†àÏù∏'), (12, 13, '')\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Calculating BLEU:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 810/1000 [08:53<02:03,  1.53it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Error parsing edit spans: invalid syntax. Perhaps you forgot a comma? (<unknown>, line 1)\nraw_edit_spans: (4 4, 'Ïûà'), (10, 11, ' Î®πÏóàÏñ¥.')\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Calculating BLEU:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 851/1000 [09:20<01:40,  1.48it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Error parsing edit spans: invalid syntax (<unknown>, line 1)\nraw_edit_spans: (3,, '.'), (11, 12, ' Í±∞'), (14, 14, '.')\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Calculating BLEU:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 896/1000 [09:49<01:07,  1.54it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Error parsing edit spans: invalid syntax. Perhaps you forgot a comma? (<unknown>, line 1)\nraw_edit_spans: (4 4, 'Í≤†')\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Calculating BLEU:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 912/1000 [10:00<00:58,  1.52it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Error parsing edit spans: invalid syntax. Perhaps you forgot a comma? (<unknown>, line 1)\nraw_edit_spans: (4 3, ','), (17, 17, '.')\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Calculating BLEU:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 957/1000 [10:29<00:28,  1.49it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Error parsing edit spans: invalid syntax. Perhaps you forgot a comma? (<unknown>, line 1)\nraw_edit_spans: (7 2, 'Ìïò')\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Calculating BLEU:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 962/1000 [10:33<00:25,  1.50it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Error parsing edit spans: invalid syntax. Perhaps you forgot a comma? (<unknown>, line 1)\nraw_edit_spans: (4 3, 4, '.')\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Calculating BLEU:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 972/1000 [10:39<00:18,  1.53it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Error parsing edit spans: invalid syntax (<unknown>, line 1)\nraw_edit_spans: (3,, ' '), (8, 8, '.')\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Calculating BLEU:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 975/1000 [10:41<00:16,  1.53it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Error parsing edit spans: invalid syntax (<unknown>, line 1)\nraw_edit_spans: (2,, ''), (8, 9, 'ÎÑà'), (14, 14, ' '), (15, 16, 'Îäî'), (21, 21, ' '), (22, 23, ''), (24, 25, ''), (26, 27, ''), (31, 31, '.')\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Calculating BLEU:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 977/1000 [10:42<00:15,  1.52it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Error parsing edit spans: invalid syntax (<unknown>, line 1)\nraw_edit_spans: (2,, 'Ï∞ÆÏïÑ.'), (11, 12, ''), (17, 17, '.')\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Calculating BLEU: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [10:57<00:00,  1.52it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "BLEU Score: 0.05479911274499472\nTime: 657.9792716503143\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n/opt/conda/lib/python3.10/site-packages/nltk/translate/bleu_score.py:490: UserWarning: \nCorpus/Sentence contains 0 counts of 2-gram overlaps.\nBLEU scores might be undesirable; use SmoothingFunction().\n  warnings.warn(_msg)\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "d1u6gFgFFimk"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}