{
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.14",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "sourceId": 10077893,
          "sourceType": "datasetVersion",
          "datasetId": 6212438
        },
        {
          "sourceId": 10078304,
          "sourceType": "datasetVersion",
          "datasetId": 6212762
        }
      ],
      "dockerImageVersionId": 30787,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yeeeeeun/GEC_by_edit_spans/blob/main/final_gec_propose.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from transformers import Trainer, TrainingArguments\n",
        "import json"
      ],
      "metadata": {
        "id": "Bn3nv9vmL0IC",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-02T14:15:43.366834Z",
          "iopub.execute_input": "2024-12-02T14:15:43.367516Z",
          "iopub.status.idle": "2024-12-02T14:15:43.371808Z",
          "shell.execute_reply.started": "2024-12-02T14:15:43.367482Z",
          "shell.execute_reply": "2024-12-02T14:15:43.370991Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GPU 부족으로 Kaggle 사용\n"
      ],
      "metadata": {
        "id": "XxuRYmmhFknw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "FlokLWi0NVwP",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install accelerate>=0.20.1"
      ],
      "metadata": {
        "id": "A8hkGxrZUtH1",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# https://huggingface.co/j5ng/et5-base"
      ],
      "metadata": {
        "id": "gSe4XY0FQHT_"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, BartConfig,PreTrainedTokenizerFast\n",
        "import torch\n",
        "# Load the model and tokenizer\n",
        "model_name = 'gogamza/kobart-base-v2'\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "EeamwJAtO3Ek",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "referenced_widgets": [
            "8b6b60dd03ab4e35b887554e099eb35e",
            "65fce8841045477880cef7957445d4da",
            "db87391f1966410d81484d262d816b3e",
            "39cd5c0b57934df0b6a15c6c359df246",
            "24510edbca2345d29df5f4d233e593fb"
          ]
        },
        "outputId": "a1d1b6d7-5c16-43aa-f6cd-a5f6d9ea3588",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-02T14:15:13.040955Z",
          "iopub.execute_input": "2024-12-02T14:15:13.041270Z",
          "iopub.status.idle": "2024-12-02T14:15:19.780527Z",
          "shell.execute_reply.started": "2024-12-02T14:15:13.041242Z",
          "shell.execute_reply": "2024-12-02T14:15:19.779808Z"
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "config.json:   0%|          | 0.00/1.36k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8b6b60dd03ab4e35b887554e099eb35e"
            }
          },
          "metadata": {}
        },
        {
          "name": "stderr",
          "text": "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer.json:   0%|          | 0.00/682k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "65fce8841045477880cef7957445d4da"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "added_tokens.json:   0%|          | 0.00/4.00 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "db87391f1966410d81484d262d816b3e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "39cd5c0b57934df0b6a15c6c359df246"
            }
          },
          "metadata": {}
        },
        {
          "name": "stderr",
          "text": "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\nYou passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "model.safetensors:   0%|          | 0.00/495M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "24510edbca2345d29df5f4d233e593fb"
            }
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\")"
      ],
      "metadata": {
        "id": "hXi_ZRDzQJfw",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-02T14:15:19.781717Z",
          "iopub.execute_input": "2024-12-02T14:15:19.782107Z",
          "iopub.status.idle": "2024-12-02T14:15:19.786259Z",
          "shell.execute_reply.started": "2024-12-02T14:15:19.782061Z",
          "shell.execute_reply": "2024-12-02T14:15:19.785335Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "model = model.to(device)"
      ],
      "metadata": {
        "id": "FTAVA3m4U6KS",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-02T14:15:19.787381Z",
          "iopub.execute_input": "2024-12-02T14:15:19.787725Z",
          "iopub.status.idle": "2024-12-02T14:15:20.182380Z",
          "shell.execute_reply.started": "2024-12-02T14:15:19.787689Z",
          "shell.execute_reply": "2024-12-02T14:15:20.181568Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/kaggle/input/ffffffinal-dataset/propose_processed_data.json', \"r\", encoding=\"utf-8\") as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "df_sampled = df.sample(n=30000, random_state=42)\n",
        "\n",
        "x = df_sampled['input']\n",
        "y = df_sampled['output']\n",
        "\n",
        "train_df, test_df = train_test_split(df_sampled, train_size=20000, test_size=10000, random_state=42)\n",
        "train_df, val_df = train_test_split(train_df, test_size=0.2, random_state=42)\n",
        "\n",
        "x_train = train_df['input']\n",
        "x_valid = val_df['input']\n",
        "y_train = train_df['output']\n",
        "y_valid = val_df['output']\n",
        "\n",
        "print(\"Train 데이터 크기:\", train_df.shape[0])\n",
        "print(\"Valid 데이터 크기:\", val_df.shape[0])\n",
        "print(\"Test 데이터 크기:\", test_df.shape[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hm775KnlQfGM",
        "outputId": "58694614-5f48-4e6e-87d3-23797f7e3ff7",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-02T16:35:16.784195Z",
          "iopub.execute_input": "2024-12-02T16:35:16.784562Z",
          "iopub.status.idle": "2024-12-02T16:35:17.380885Z",
          "shell.execute_reply.started": "2024-12-02T16:35:16.784532Z",
          "shell.execute_reply": "2024-12-02T16:35:17.379976Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Train 데이터 크기: 16000\nValid 데이터 크기: 4000\nTest 데이터 크기: 10000\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "KreNvbiPVOFv",
        "outputId": "8efb297e-b140-44a7-a90e-d2e27b3e56b9",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-02T14:22:59.893499Z",
          "iopub.execute_input": "2024-12-02T14:22:59.893845Z",
          "iopub.status.idle": "2024-12-02T14:22:59.900671Z",
          "shell.execute_reply.started": "2024-12-02T14:22:59.893808Z",
          "shell.execute_reply": "2024-12-02T14:22:59.899795Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "                        input                                     output\n118839    꼭 아침에 밥 먹고 알바가리~!!!  (13, 13, ' '), (14, 15, ''), (17, 19, '')\n79677   아몬드 페페는 이 삼일에 주라고 하드라  (9, 10, ''), (19, 20, '더'), (21, 21, '.')\n215391                등등 많이주는                                (5, 5, ' ')\n78682                 야채죽...?                                 (3, 6, '')\n176584                     네네                                (2, 2, '.')\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "max_length = df_sampled.iloc[:, 0].str.len().max()\n",
        "\n",
        "\n",
        "longest_sentence = df_sampled.loc[df_sampled.iloc[:, 0].str.len() == max_length, df_sampled.columns[0]].values[0]\n",
        "\n",
        "print(\"가장 긴 문장의 글자수:\", max_length)\n",
        "print(\"가장 긴 문장을 출력함:\", longest_sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c50eSX6TVSx3",
        "outputId": "4418b05a-b67a-416e-a20a-19fae3903a0b",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-02T14:23:00.806194Z",
          "iopub.execute_input": "2024-12-02T14:23:00.806514Z",
          "iopub.status.idle": "2024-12-02T14:23:00.838786Z",
          "shell.execute_reply.started": "2024-12-02T14:23:00.806489Z",
          "shell.execute_reply": "2024-12-02T14:23:00.837952Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "가장 긴 문장의 글자수: 477\n가장 긴 문장을 출력함: 그러면서도 자기가 딸에게 다시 잘이야기 해보신다며 여운을 남기시길래 집오면서 도와달라고 말씀을 드렸고, 다음날엔 저렇게 카톡을 보내시길래 제가 이런 문자를 보냈거든\n네. 어머니 말씀 잘이해했습니다. 어제 제게 일말의 여운을 남기지 않으셨다면 하루더빨리 마음정리 할 수 있었을껍니다.  어머니께서 좋은마음 갖고 도와주시겠다고 하시어 드린말씀이지 집착은 아니었습니다. 매주 따님과 저희 집에서 함께지내고, 또 매일 붙어 있다싶이 했습니다. 때론 어머니께 거짓말을하고 여행을 가기도 했습니다. 이젠 서로 마음이 깊어졌고 그 깊어진 마음 보여드리려했을뿐 현실적인것을 말씀드리려한건 아닙니다. 돈이 중요한거 알고 안정적인 생활이 중요한건 압니다. 제가 부모님께 도움을 받았으면 받았지 드리는 형편아닙니다. 부모님은 언론인 출신이시고 공무원생활도 오래하신 분들 입니다. 현실보다 감정이 기반된 사랑을 하고싶었을 뿐이에요. 아쉽지만 저는 이만 하렵니다. 잘지내세요.\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "class GECDatasets(Dataset):\n",
        "    def __init__(self, data, tokenizer, max_length=128):\n",
        "        self.data = data\n",
        "        self.input = [example['input'] for example in data]\n",
        "        self.output = [example['output'] for example in data]  # 수정 정보\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        input_text = self.input[idx]\n",
        "        edits = self.output[idx]\n",
        "\n",
        "\n",
        "        encoding = self.tokenizer(input_text, truncation=True, padding='max_length', max_length=self.max_length, return_tensors='pt')\n",
        "        input_ids = encoding['input_ids'].squeeze(0)\n",
        "        attention_mask = encoding['attention_mask'].squeeze(0)\n",
        "\n",
        "        output_encoding = self.tokenizer(edits, truncation=True, padding='max_length', max_length=self.max_length, return_tensors='pt')\n",
        "        labels = output_encoding['input_ids'].squeeze(0)\n",
        "\n",
        "        sample = {\n",
        "            'input_ids': input_ids,\n",
        "            'attention_mask': attention_mask,\n",
        "            'labels': labels\n",
        "        }\n",
        "\n",
        "        return sample\n"
      ],
      "metadata": {
        "id": "HKfiydB0Y4rs",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-02T14:23:02.548968Z",
          "iopub.execute_input": "2024-12-02T14:23:02.549585Z",
          "iopub.status.idle": "2024-12-02T14:23:02.556822Z",
          "shell.execute_reply.started": "2024-12-02T14:23:02.549554Z",
          "shell.execute_reply": "2024-12-02T14:23:02.555896Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = GECDatasets(data=df_sampled.to_dict(orient='records'), tokenizer=tokenizer)"
      ],
      "metadata": {
        "id": "haNSOJBeY6Ax",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-02T14:23:04.298424Z",
          "iopub.execute_input": "2024-12-02T14:23:04.299236Z",
          "iopub.status.idle": "2024-12-02T14:23:04.390214Z",
          "shell.execute_reply.started": "2024-12-02T14:23:04.299203Z",
          "shell.execute_reply": "2024-12-02T14:23:04.389273Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0XMZs5DVg-T0",
        "outputId": "b16617d0-173d-4ef5-909e-bc8397b04463",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-02T14:23:05.626384Z",
          "iopub.execute_input": "2024-12-02T14:23:05.626756Z",
          "iopub.status.idle": "2024-12-02T14:23:05.635222Z",
          "shell.execute_reply.started": "2024-12-02T14:23:05.626727Z",
          "shell.execute_reply": "2024-12-02T14:23:05.634331Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "{'input_ids': tensor([14427,  9866, 20141, 12258, 14304,   214,     3,     3,     3,     3,\n            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n            3,     3,     3,     3,     3,     3,     3,     3]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0]), 'labels': tensor([14338,   255,   243, 14260,   243, 14063,   245, 14063,   240,     3,\n            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n            3,     3,     3,     3,     3,     3,     3,     3])}\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(dataset))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0W33OuKJnJZR",
        "outputId": "2275e6c8-7097-46c0-d2f6-307afdbc5211",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-02T14:23:14.411698Z",
          "iopub.execute_input": "2024-12-02T14:23:14.412064Z",
          "iopub.status.idle": "2024-12-02T14:23:14.416698Z",
          "shell.execute_reply.started": "2024-12-02T14:23:14.412033Z",
          "shell.execute_reply": "2024-12-02T14:23:14.415897Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "30000\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = []\n",
        "for input_text, edits in zip(x_train.values, y_train.values):\n",
        "    train_data.append({\n",
        "        'input': input_text,  # 잘못된 문장\n",
        "        'output': edits        # 수정 정보\n",
        "    })\n",
        "\n",
        "valid_data = []\n",
        "for input_text, edits in zip(x_valid.values, y_valid.values):\n",
        "    valid_data.append({\n",
        "        'input': input_text,  # 잘못된 문장\n",
        "        'output': edits        # 수정 정보\n",
        "    })\n",
        "\n",
        "train_dataset = GECDatasets(train_data, tokenizer)\n",
        "valid_dataset = GECDatasets(valid_data, tokenizer)"
      ],
      "metadata": {
        "id": "9B3N5-tHjQ6t",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-02T14:23:22.792001Z",
          "iopub.execute_input": "2024-12-02T14:23:22.792837Z",
          "iopub.status.idle": "2024-12-02T14:23:22.814897Z",
          "shell.execute_reply.started": "2024-12-02T14:23:22.792804Z",
          "shell.execute_reply": "2024-12-02T14:23:22.814102Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import EarlyStoppingCallback\n",
        "\n",
        "# EarlyStoppingCallback 설정\n",
        "early_stopping = EarlyStoppingCallback(\n",
        "    early_stopping_patience=2,\n",
        "    early_stopping_threshold=0.0,\n",
        ")\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=5e-5,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    num_train_epochs=10,\n",
        "    weight_decay=0.01,\n",
        "    save_total_limit=1,\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=50,\n",
        "    report_to=\"none\",\n",
        "    load_best_model_at_end=True,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=valid_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    callbacks=[early_stopping],\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 167
        },
        "id": "ELftuDVpjKG7",
        "outputId": "f21381ef-e9ef-4b45-bc24-cb077b51003a",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-02T14:24:08.706371Z",
          "iopub.execute_input": "2024-12-02T14:24:08.706742Z",
          "iopub.status.idle": "2024-12-02T15:00:38.338623Z",
          "shell.execute_reply.started": "2024-12-02T14:24:08.706709Z",
          "shell.execute_reply": "2024-12-02T15:00:38.337633Z"
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='12000' max='20000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [12000/20000 36:27 < 24:18, 5.48 it/s, Epoch 6/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.078800</td>\n      <td>0.076171</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.067400</td>\n      <td>0.065905</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.053900</td>\n      <td>0.060678</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.043300</td>\n      <td>0.058898</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.032700</td>\n      <td>0.060018</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>0.026800</td>\n      <td>0.065547</td>\n    </tr>\n  </tbody>\n</table><p>"
          },
          "metadata": {}
        },
        {
          "name": "stderr",
          "text": "/opt/conda/lib/python3.10/site-packages/transformers/modeling_utils.py:2618: UserWarning: Moving the following attributes in the config to the generation config: {'forced_eos_token_id': 1}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n  warnings.warn(\nThere were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.decoder.embed_tokens.weight', 'lm_head.weight'].\n",
          "output_type": "stream"
        },
        {
          "execution_count": 27,
          "output_type": "execute_result",
          "data": {
            "text/plain": "TrainOutput(global_step=12000, training_loss=0.06755638842781385, metrics={'train_runtime': 2188.2691, 'train_samples_per_second': 73.117, 'train_steps_per_second': 9.14, 'total_flos': 7316837498880000.0, 'train_loss': 0.06755638842781385, 'epoch': 6.0})"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "wrong_input = '그는 사과을 먹고있다.'\n",
        "\n",
        "input_ids = tokenizer(wrong_input, return_tensors='pt').input_ids\n",
        "edit_out = model.generate(input_ids, max_length=128, num_beams=4, length_penalty=2.0, early_stopping=True)\n",
        "corrected_output = tokenizer.decode(edit_out[0], skip_special_tokens=True)\n",
        "\n",
        "edits = get_edit_operations(wrong_input, corrected_output)\n",
        "print(edits)"
      ],
      "metadata": {
        "id": "_qwsI7-YjpQq"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), './final_gec_model')"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-02T15:02:14.875162Z",
          "iopub.execute_input": "2024-12-02T15:02:14.875763Z",
          "iopub.status.idle": "2024-12-02T15:02:15.585449Z",
          "shell.execute_reply.started": "2024-12-02T15:02:14.875729Z",
          "shell.execute_reply": "2024-12-02T15:02:15.584457Z"
        },
        "id": "iyR3L39lFimj"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from IPython.display import FileLink\n",
        "import os\n",
        "\n",
        "result = np.array([0.0])\n",
        "np.save(\"/kaggle/working/embedding\",result)\n",
        "os.chdir(r'/kaggle/working')\n",
        "FileLink(r'./final_gec_model')"
      ],
      "metadata": {
        "trusted": true,
        "id": "hcKnGSa9Fimj"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import BartForConditionalGeneration, BartTokenizer\n",
        "\n",
        "model_name = 'gogamza/kobart-base-v2'\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "\n",
        "model.load_state_dict(torch.load('/kaggle/working/final_gec_model', map_location=torch.device('cpu')))\n",
        "model.eval()\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "\n",
        "print(\"문장을 입력하세요:\")\n",
        "input_text = input()\n",
        "\n",
        "input_encoding = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
        "\n",
        "output_encoding = model.generate(\n",
        "    input_ids=input_encoding.input_ids,\n",
        "    attention_mask=input_encoding.attention_mask,\n",
        "    max_length=128,\n",
        "    num_beams=5,\n",
        ")\n",
        "\n",
        "output_text = tokenizer.decode(output_encoding[0], skip_special_tokens=True)\n",
        "\n",
        "print(\"edit spans: \", output_text)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-02T16:49:36.625027Z",
          "iopub.execute_input": "2024-12-02T16:49:36.625724Z",
          "iopub.status.idle": "2024-12-02T16:49:46.054630Z",
          "shell.execute_reply.started": "2024-12-02T16:49:36.625692Z",
          "shell.execute_reply": "2024-12-02T16:49:46.053699Z"
        },
        "id": "zwSD9r8hFimj",
        "outputId": "9de658e3-a31d-415f-f80a-d41d5df4b726"
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\nYou passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\nYou passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n/tmp/ipykernel_30/3546478252.py:10: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load('/kaggle/working/final_gec_model', map_location=torch.device('cpu')))\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "문장을 입력하세요:\n",
          "output_type": "stream"
        },
        {
          "output_type": "stream",
          "name": "stdin",
          "text": " 왔어용?\n"
        },
        {
          "name": "stdout",
          "text": "edit spans:  (2, 3, '요')\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install m2-score\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-02T16:27:01.610485Z",
          "iopub.execute_input": "2024-12-02T16:27:01.611308Z",
          "iopub.status.idle": "2024-12-02T16:27:03.418082Z",
          "shell.execute_reply.started": "2024-12-02T16:27:01.611275Z",
          "shell.execute_reply": "2024-12-02T16:27:03.417082Z"
        },
        "id": "SsfIzsz1Fimj",
        "outputId": "7ee60ff0-d043-4f34-e2f9-8894d9509550"
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\u001b[31mERROR: Could not find a version that satisfies the requirement m2-score (from versions: none)\u001b[0m\u001b[31m\n\u001b[0m\u001b[31mERROR: No matching distribution found for m2-score\u001b[0m\u001b[31m\n\u001b[0m",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from tqdm import tqdm\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "import pandas as pd\n",
        "import time\n",
        "import ast\n",
        "\n",
        "model_name = 'gogamza/kobart-base-v2'\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "\n",
        "model.load_state_dict(torch.load('/kaggle/working/final_gec_model', map_location=torch.device('cpu')))\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "test_df = test_df.sample(n=1000, random_state=42)\n",
        "\n",
        "def apply_edit_spans(input_sentence, edit_spans):\n",
        "\n",
        "    input_sentence = list(input_sentence)\n",
        "    try:\n",
        "        for span in sorted(edit_spans, key=lambda x: x[0], reverse=True):\n",
        "            start, end, replacement = span\n",
        "            input_sentence[start:end] = list(replacement)\n",
        "    except Exception as e:\n",
        "        print(f\"Error in apply_edit_spans: {e}\")\n",
        "        print(f\"edit_spans: {edit_spans}\")\n",
        "        raise e\n",
        "    return ''.join(input_sentence)\n",
        "\n",
        "def calculate_bleu(model, tokenizer, dataset, max_length=128):\n",
        "    references = []\n",
        "    hypotheses = []\n",
        "\n",
        "    start_time = time.time()\n",
        "    with torch.no_grad():\n",
        "        for _, row in tqdm(dataset.iterrows(), desc=\"Calculating BLEU\", total=len(dataset)):\n",
        "            source_sentence = row['input']\n",
        "            target_sentence = row['output']\n",
        "\n",
        "            inputs = tokenizer(source_sentence, return_tensors=\"pt\", padding=True, truncation=True, max_length=max_length).to(device)\n",
        "\n",
        "            output = model.generate(input_ids=inputs['input_ids'], attention_mask=inputs['attention_mask'], max_length=max_length)\n",
        "            raw_edit_spans = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "            try:\n",
        "                edit_spans = ast.literal_eval(f\"[{raw_edit_spans}]\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error parsing edit spans: {e}\")\n",
        "                print(f\"raw_edit_spans: {raw_edit_spans}\")\n",
        "                continue\n",
        "\n",
        "            corrected_sentence = apply_edit_spans(source_sentence, edit_spans)\n",
        "\n",
        "            references.append([target_sentence.split()])\n",
        "            hypotheses.append(corrected_sentence.split())\n",
        "\n",
        "    prediction_time = time.time() - start_time\n",
        "    bleu_score = corpus_bleu(references, hypotheses)\n",
        "    return bleu_score, prediction_time\n",
        "\n",
        "bleu_score, prediction_time = calculate_bleu(model, tokenizer, test_df)\n",
        "print(f\"BLEU Score: {bleu_score}\")\n",
        "print(f\"Time: {prediction_time}\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-02T16:35:20.988698Z",
          "iopub.execute_input": "2024-12-02T16:35:20.989079Z",
          "iopub.status.idle": "2024-12-02T16:46:21.285474Z",
          "shell.execute_reply.started": "2024-12-02T16:35:20.989048Z",
          "shell.execute_reply": "2024-12-02T16:46:21.284583Z"
        },
        "id": "1qjnEe2dFimk",
        "outputId": "5f59a8df-16c3-4ab9-b4cd-a0c9419732ba"
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\nYou passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\nYou passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n/tmp/ipykernel_30/2550538412.py:15: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load('/kaggle/working/final_gec_model', map_location=torch.device('cpu')))\nCalculating BLEU:   4%|▍         | 38/1000 [00:25<10:31,  1.52it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Error parsing edit spans: invalid syntax (<unknown>, line 1)\nraw_edit_spans: (3,, ' '), (15, 15, ' '), (17, 17, ' '), (22, 22, '. ')\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Calculating BLEU:   8%|▊         | 76/1000 [00:50<10:11,  1.51it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Error parsing edit spans: invalid syntax (<unknown>, line 1)\nraw_edit_spans: (3,, ' '), (4, 4, ' '), (6, 6, ' '), (11, 11, ' '), (13, 13, ' '), (16, 16, ' '), (17, 17, '.')\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Calculating BLEU:  13%|█▎        | 126/1000 [01:22<09:39,  1.51it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Error parsing edit spans: invalid syntax (<unknown>, line 1)\nraw_edit_spans: (3,, ' '), (5, 6, ''), (7, 7, '.')\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Calculating BLEU:  30%|██▉       | 299/1000 [03:16<07:41,  1.52it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Error parsing edit spans: closing parenthesis ']' does not match opening parenthesis '(' (<unknown>, line 1)\nraw_edit_spans: (3, 4, '샀'), (6, 7, ' 있'), (8, 8, '.'), (11, 11, ' '), (13, 13, ' '), (15, 15, ' '), (17, 17, ' '), (22, 22, ' '), (24, 24, ' '), (26, 26, ' '), (27, 27, ' '), (31, 31, ' '), (32, 32, ' '), (34, 34, ' '), (36, 36, ' '), (36, 36\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Calculating BLEU:  30%|███       | 300/1000 [03:17<07:39,  1.52it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Error parsing edit spans: unterminated string literal (detected at line 1) (<unknown>, line 1)\nraw_edit_spans: (3, 4, '3, 4, '는'), (5, 5, ' '), (7, 7, '.')\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Calculating BLEU:  31%|███▏      | 313/1000 [03:26<07:29,  1.53it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Error parsing edit spans: invalid syntax (<unknown>, line 1)\nraw_edit_spans: (3,, ' '), (6, 6, '.')\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Calculating BLEU:  33%|███▎      | 334/1000 [03:40<07:18,  1.52it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Error parsing edit spans: invalid syntax (<unknown>, line 1)\nraw_edit_spans: (1,, ' 거'), (3, 4, ' 거'), (5, 5, '.')\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Calculating BLEU:  34%|███▍      | 344/1000 [03:46<07:04,  1.55it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Error parsing edit spans: invalid syntax (<unknown>, line 1)\nraw_edit_spans: (3,, ' ')\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Calculating BLEU:  48%|████▊     | 477/1000 [05:13<05:42,  1.53it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Error parsing edit spans: invalid syntax (<unknown>, line 1)\nraw_edit_spans: (3,, '.')\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Calculating BLEU:  51%|█████     | 512/1000 [05:37<05:19,  1.53it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Error parsing edit spans: invalid syntax. Perhaps you forgot a comma? (<unknown>, line 1)\nraw_edit_spans: (5 2, '찮'), (6, 6, ',')\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Calculating BLEU:  54%|█████▎    | 537/1000 [05:53<05:01,  1.53it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Error parsing edit spans: invalid syntax. Perhaps you forgot a comma? (<unknown>, line 1)\nraw_edit_spans: (4 3, ','), (7, 7, ' ')\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Calculating BLEU:  54%|█████▍    | 541/1000 [05:56<05:00,  1.53it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Error parsing edit spans: invalid syntax. Perhaps you forgot a comma? (<unknown>, line 1)\nraw_edit_spans: (7 2, '발'), (7, 7, ' '), (9, 9, '.')\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Calculating BLEU:  56%|█████▌    | 557/1000 [06:06<04:49,  1.53it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Error parsing edit spans: invalid syntax (<unknown>, line 1)\nraw_edit_spans: (1,, '웃'), (14, 14, ' '), (17, 18, '')\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Calculating BLEU:  60%|█████▉    | 599/1000 [06:34<04:20,  1.54it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Error parsing edit spans: invalid syntax. Perhaps you forgot a comma? (<unknown>, line 1)\nraw_edit_spans: (4 4, '')\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Calculating BLEU:  60%|██████    | 604/1000 [06:37<04:21,  1.51it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Error parsing edit spans: invalid syntax (<unknown>, line 1)\nraw_edit_spans: (3,, ' '), (4, 4, ' '), (22, 22, '.')\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Calculating BLEU:  63%|██████▎   | 631/1000 [06:55<04:00,  1.53it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Error parsing edit spans: invalid syntax (<unknown>, line 1)\nraw_edit_spans: (1,, ''), (22, 22, ' ')\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Calculating BLEU:  67%|██████▋   | 667/1000 [07:19<03:35,  1.55it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Error parsing edit spans: invalid syntax (<unknown>, line 1)\nraw_edit_spans: (2,, '러'), (27, 27, '.')\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Calculating BLEU:  68%|██████▊   | 680/1000 [07:27<03:27,  1.54it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Error parsing edit spans: invalid syntax (<unknown>, line 1)\nraw_edit_spans: (2,, '')\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Calculating BLEU:  72%|███████▏  | 719/1000 [07:53<03:02,  1.54it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Error parsing edit spans: invalid syntax (<unknown>, line 1)\nraw_edit_spans: (3,, '.')\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Calculating BLEU:  75%|███████▌  | 754/1000 [08:16<02:42,  1.52it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Error parsing edit spans: invalid syntax (<unknown>, line 1)\nraw_edit_spans: (2,, ''), (22, 22, ' ')\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Calculating BLEU:  80%|████████  | 802/1000 [08:47<02:10,  1.52it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Error parsing edit spans: invalid syntax (<unknown>, line 1)\nraw_edit_spans: (1,, '레인'), (12, 13, '')\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Calculating BLEU:  81%|████████  | 810/1000 [08:53<02:03,  1.53it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Error parsing edit spans: invalid syntax. Perhaps you forgot a comma? (<unknown>, line 1)\nraw_edit_spans: (4 4, '있'), (10, 11, ' 먹었어.')\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Calculating BLEU:  85%|████████▌ | 851/1000 [09:20<01:40,  1.48it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Error parsing edit spans: invalid syntax (<unknown>, line 1)\nraw_edit_spans: (3,, '.'), (11, 12, ' 거'), (14, 14, '.')\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Calculating BLEU:  90%|████████▉ | 896/1000 [09:49<01:07,  1.54it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Error parsing edit spans: invalid syntax. Perhaps you forgot a comma? (<unknown>, line 1)\nraw_edit_spans: (4 4, '겠')\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Calculating BLEU:  91%|█████████ | 912/1000 [10:00<00:58,  1.52it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Error parsing edit spans: invalid syntax. Perhaps you forgot a comma? (<unknown>, line 1)\nraw_edit_spans: (4 3, ','), (17, 17, '.')\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Calculating BLEU:  96%|█████████▌| 957/1000 [10:29<00:28,  1.49it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Error parsing edit spans: invalid syntax. Perhaps you forgot a comma? (<unknown>, line 1)\nraw_edit_spans: (7 2, '하')\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Calculating BLEU:  96%|█████████▌| 962/1000 [10:33<00:25,  1.50it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Error parsing edit spans: invalid syntax. Perhaps you forgot a comma? (<unknown>, line 1)\nraw_edit_spans: (4 3, 4, '.')\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Calculating BLEU:  97%|█████████▋| 972/1000 [10:39<00:18,  1.53it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Error parsing edit spans: invalid syntax (<unknown>, line 1)\nraw_edit_spans: (3,, ' '), (8, 8, '.')\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Calculating BLEU:  98%|█████████▊| 975/1000 [10:41<00:16,  1.53it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Error parsing edit spans: invalid syntax (<unknown>, line 1)\nraw_edit_spans: (2,, ''), (8, 9, '너'), (14, 14, ' '), (15, 16, '는'), (21, 21, ' '), (22, 23, ''), (24, 25, ''), (26, 27, ''), (31, 31, '.')\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Calculating BLEU:  98%|█████████▊| 977/1000 [10:42<00:15,  1.52it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Error parsing edit spans: invalid syntax (<unknown>, line 1)\nraw_edit_spans: (2,, '찮아.'), (11, 12, ''), (17, 17, '.')\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Calculating BLEU: 100%|██████████| 1000/1000 [10:57<00:00,  1.52it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "BLEU Score: 0.05479911274499472\nTime: 657.9792716503143\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n/opt/conda/lib/python3.10/site-packages/nltk/translate/bleu_score.py:490: UserWarning: \nCorpus/Sentence contains 0 counts of 2-gram overlaps.\nBLEU scores might be undesirable; use SmoothingFunction().\n  warnings.warn(_msg)\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "d1u6gFgFFimk"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}